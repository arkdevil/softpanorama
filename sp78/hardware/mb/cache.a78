From: tatosian@plough.enet.dec.com (Dave Tatosian)
Newsgroups: comp.sys.ibm.pc.hardware.chips
Subject: Re: A 128K L2 cache upgrade increases Norton SI from 94 to 95. Comments?
Date: 1 Jun 1995 06:41:40 GMT

In article <3qgmal$64u@expert.cc.purdue.edu>,
   ddonne@expert.cc.purdue.edu (David Donne) wrote:
>I added 128K cache memory to my 486 DX2/50 PC. Norton SI score went from
>94 to 95 only. What happened?
>
>      System configuration:
>
>          Packard Bell 486DX2/50
>          8 MB RAM
>          420 MB HD
>          Cirrus Logic 5424 video card integrated to the MB (512K)
>          0 K L2 cache before upgrade
>
>Note: When I ran Mathematica, I did see a slight improvement. One particular
>job required 44 sec of CPU time. Now it can be done within 40 sec.
>
>My question:
>
>     How much improvement can one expect when external cache memory goes
>from 0 to 128 K? Also, a friend told me I need 32 K cache for each megabyte
>of DRAM. Is that true?

Well, you've got some friend there, that's for sure - although I'd *really*
love to have (and be able to afford) 32K of cache for each of my 64MB of main
memory (whee doggies!)

The short answer is that even if you *wanted* that much cache, you can't have
it. Nobody builds mainboards or chipsets that can scale the cache to that
extend. You can only add cache in the fixed increments that the mainboard
allows - which should be described in the system or mainboard user manual. In
your case it's likely that you can have 64KB, 128KB, and even 256KB, with the
right parts installed (but again, RTFM!)

Regarding Norton SI: isn't that a composite score for the entire machine
(including disks)? Although even so, it seems strange that it would only bump
up by ~1% (which is probably within a deviation factor anyway).

Presuming that you set any jumpers as required when you fitted the cache, did
you also go into the system setup and enable the external cache? (just
checking - you'd be surprised how often things like that get forgotten)...


<><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
<> Dave Tatosian           tatosian@plough.enet.dec.com <>
<> Digital Equipment Corp.    Alpha Server Engineering  <>
<>           "Read this and nobody gets hurt"           <>
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><>

From ankh.iia.org!babbage.ece.uc.edu!ucunix.san.uc.edu!ucbeh!gw2.att.com!rutgers!news.iag.net!usenet.eel.ufl.edu!gatech!newsfeed.pitt.edu!blust Fri Jun  2 23:19:31 1995
Path: ankh.iia.org!babbage.ece.uc.edu!ucunix.san.uc.edu!ucbeh!gw2.att.com!rutgers!news.iag.net!usenet.eel.ufl.edu!gatech!newsfeed.pitt.edu!blust
Newsgroups: comp.sys.ibm.pc.hardware.chips
Subject: Re: Pipelined SRAM cache (PC-Magazine article)
Message-ID: <3qh2tf$il4@usenet.srv.cis.pitt.edu>
From: blust+@pitt.edu (Bordir Luoh)
Date: 31 May 1995 06:38:39 GMT
References: <3qcm3q$a7c@umcc.umcc.umich.edu>
 <3qdcb1$5th@usenet.srv.cis.pitt.edu> <3qeild$lmk@peavax.eng.pko.dec.com>
Organization: University of Pittsburgh
NNTP-Posting-Host: unixs4.cis.pitt.edu
X-Newsreader: TIN [version 1.2 PL2]
Lines: 62

: [snip]

: Geeze...Where to begin?

: First of all, your belief that a 15ns SRAM can actually allow a 66MHZ system
: to run with zero (or even one) wait states is naive/uninformed. You've
: completely discounted any delays contributed by signal etch, and let me tell
: you, as someone who has to wear a signal integrity engineer's hat more often
: than I like to, there's no such critter as an etch with zero delay.

The cache access time(15ns) has included reasonable delay.
Actually, it is better than 15ns when the spec said it is
15ns. The spec always tells you the worst case.

: Second, you apparently have no clue as to the functional differences between
: synchronous and asynchronous SRAMs, and what these differences mean to a
: system designer. I've already posted a writeup on the benefits of pipelining -
: perhaps your newsreader will allow you to search back a few days to find it.
: Suffice to say that there is a significant benefit - of at minimum 3 wait
: states per L1 cache line fill from the L2 cache - with sync SRAMs vs async
: SRAMS of the same "speed" (3-1-1-1 vs 3-2-2-2 for example). A 5 tick line fill
: vs. 9 is a large difference where I come from.

We use SRAM as the L2 cache because we want the system maintains
zero or one wait state when having L2 cache hits (or we should
say, the majority in the memory hierarchy).
Long story. Before 386DX25, we don't need SRAM as cache. Since
1/25MHz = 40ns and DRAM access time was about 100ns then, we could not
maintain 0/1 wait state. Therefore, every computer after that
generation use SRAM (25ns then).
Go back a little bit further, 8MHz 80286 (cycle time = 125ns) was
happy with 150 ns DRAM.
Also, the original post concentrates on the comparison of syn cache
vs. asyn cache. I only consider L2 cache hit in my analysis.
When there are cache miss, we must deal with both SRAM and DRAM.
The slower DRAM becomes bottleneck.
2-1-1-1 and 3-2-2-2 are two common conventional DRAM/SRAM arrangement.
The first number indicates the clock cycles necessary to deliver
the deliverthe initial line of data. The remaining numbers are the
cycles required for each additional line. They are not between L1/L2
cache.

: And third (and perhaps most important) I wouldn't rely on ANYTHING that PC
: Magazine has to say. They've demonstrated not only significant bias in their
: ratings (often it's pure garbage - especially their Editor's Choices - which
: are often not supported by their own benchmarking) but the benchmarking that
: they do is often incorrectly performed (their review this month of the HP/CMS
: T1000 contains the most recent of their screw-ups) and/or the results are
: misinterpreted.

: I suggest that you totally ignore their recommendations and awards, and take
: the rest of what they do with a ton of salt. If you want to find a magazine
: that knows how to perform bencmarking, and has the cumulative intelligence to
: interpret their results, learn how to read German and subscribe to C't...

No comment here.

: <><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
: <> Dave Tatosian           tatosian@plough.enet.dec.com <>
: <> Digital Equipment Corp.    Alpha Server Engineering  <>
: <>           "Read this and nobody gets hurt"           <>
: <><><><><><><><><><><><><><><><><><><><><><><><><><><><><>

From ankh.iia.org!uunet!news.mathworks.com!uhog.mit.edu!bloom-beacon.mit.edu!world!decwrl!pa.dec.com!jac.zko.dec.com!peavax.eng.pko.dec.com!te175 Fri Jun  2 23:20:46 1995
Path: ankh.iia.org!uunet!news.mathworks.com!uhog.mit.edu!bloom-beacon.mit.edu!world!decwrl!pa.dec.com!jac.zko.dec.com!peavax.eng.pko.dec.com!te175
From: tatosian@plough.enet.dec.com (Dave Tatosian)
Newsgroups: comp.sys.ibm.pc.hardware.chips
Subject: Re: Pipelined SRAM cache (PC-Magazine article)
Date: 1 Jun 1995 08:12:23 GMT
Organization: DEC/Alpha Server Engineering
Lines: 86
Message-ID: <3qjsp7$7g6@peavax.eng.pko.dec.com>
References: <3qcm3q$a7c@umcc.umcc.umich.edu> <3qdcb1$5th@usenet.srv.cis.pitt.edu> <3qeild$lmk@peavax.eng.pko.dec.com> <3qh2tf$il4@usenet.srv.cis.pitt.edu>
NNTP-Posting-Host: te175.pko.dec.com
X-Newsreader: News Xpress Version 1.0 Beta #3

In article <3qh2tf$il4@usenet.srv.cis.pitt.edu>,
   blust+@pitt.edu (Bordir Luoh) wrote:

>The cache access time(15ns) has included reasonable delay.
>Actually, it is better than 15ns when the spec said it is
>15ns. The spec always tells you the worst case.

Just so you understand the requirement: fitting a cache read into a 66MHZ
mainboard clock with zero wait states means that from the input clock to the
Pentium, to the address output latches in the Pentium, to the address output
pin drivers Pentium, to the address pins of the Pentium, to the address inputs
of the SRAM, *to the data output pins of the SRAM*, to the data input pins of
the Pentium, to the data input latches of the Pentium, has to be traversed in
15ns...Lot's of luck! Your "better than 15ns" SRAM only covers the one delay
component highlighted with the asterisks...

Beyond that startling analysis ;^) the concept of using a part beyond the AC
performance that the manufacturer guarantees it is completely foreign to me.
Daddy (and my company) always taught me that to design reliable systems, I
should respect the vendor guaranteed performance at the component level. You
can call it "worse case" if you like, but the component vendor spec's actually
list their guaranteed AC performance across the commercial operating range,
and then list a NOT GUARANTEED "typical" AC performance data point associated
with nominal VDD and room temperature. Just try to get a vendor to guarantee
AC performance at a narrower operating range than what is listed...

Your statement typifies the "overclocking" mindset - which is fine if you're
willing to run the risk that the system won't work (or might, if you're
lucky). I don't have - or desire - that luxury. When I design a system I'm
putting my personal reputation - as well as the corporation's reputation - on
the line, that the company will be able to use industry standard parts from
any vendor and build a million or more systems, and have every one of them
work, whether the system sits in a glass walled computer room with excellent
climate control, or if it sits on a factory floor in the hottest location
fathomable...

btw: Your total discounting of the AC effects of motherboard etch tells me
that high speed circuit design is not your strong suit. Signal etch,
connectors, and even package pins are quickly becoming the brick wall impeding
even faster circuit design. Discounting the effects of any of these went out
with that 286 chip...

>We use SRAM as the L2 cache because we want the system maintains
>zero or one wait state when having L2 cache hits (or we should
>say, the majority in the memory hierarchy).

Uh - "we'd" love it if "we" could get an L2 cache hit response in 0 wait
states - at 66MHZ. Unfortunately it isn't happening (see paragraph 1)..

[irrelevent history of low-frequency CPU's deleted]

>Also, the original post concentrates on the comparison of syn cache
>vs. asyn cache. I only consider L2 cache hit in my analysis.
>When there are cache miss, we must deal with both SRAM and DRAM.
>The slower DRAM becomes bottleneck.
>2-1-1-1 and 3-2-2-2 are two common conventional DRAM/SRAM arrangement.
>The first number indicates the clock cycles necessary to deliver
>the deliverthe initial line of data. The remaining numbers are the
>cycles required for each additional line. They are not between L1/L2
>cache.

Uh - not sure that I understand your point (or if there is a point there to
find). The question was whether the sync/burst cache delivers significant
performance gains over async cache, using the same mainboard operating
frequency. If you really analyze the L2 hit, then the answer - imho - is that
the sync/burst cache will deliver a cache line of data to the processor in 6
ticks, vs. 9 ticks for the async cache, and thus is considerably faster.

Do you have a counterpoint to that - or does that paragraph of yours agree?

By the way, I'm sure we'd also *love* to have a PC do main memory reads
occuring at the 2-1-1-1 or 3-2-2-2 wait states that you refer to. Again, at 60
or 66MHZ mainboard operation, that unfortunately ain't happening either: try
something closer to 7-3-3-3 (FPM) or 7-2-2-2 (EDO) as measured from
address to data for all four Quadwords in a cache line unit of data
[ref: example from a mainboard using Triton chipset]...

Hope this helps ;^)

/dave

<><><><><><><><><><><><><><><><><><><><><><><><><><><><><>
<> Dave Tatosian           tatosian@plough.enet.dec.com <>
<> Digital Equipment Corp.    Alpha Server Engineering  <>
<>           "Read this and nobody gets hurt"           <>
<><><><><><><><><><><><><><><><><><><><><><><><><><><><><>

