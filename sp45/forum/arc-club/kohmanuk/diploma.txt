.hm2  .dh1 .ce Input/Output with Embedded Data Compression
.fm64 .df1 .ce -- Page .pa --
.ls1
.pl66
.rm72 .lm0
.tm5  .bm60
.ff1
.tc 72 1 0 0  0 1 4 1 3 1 3 1 3 1
.sh .sf


.ce Дмитрий С. Кохманюк


.ce Реализация ввода-вывода со сжатием данных


.ce Input/Output with Embedded Data Compression



.ce Киев -- 1992





.ce Abstract


	The data compression techniques became very popular during the
last years. However, most compressors are implemented as standalone
applications which are hard to integrate into programs. This paper
describes input/output library with builtin compression, suitable for
embedding into applications. A quick survey of compression algorithms
is also provided.



.ce Аннотация


	В последнее время методы сжатия данных получили широкое
распространение. Тем не менее, большинство программ сжатия реализованы
в виде отдельных приложений, что затрудняет их интеграцию в программы
пользователя. В работе описывается библиотека ввода-вывода с встроенным
сжатием данных, удобную для встраивания в прикладные программы. Сделан
также краткий обзор алгоритмов сжатия.



.te 1 .ce Обзор методов сжатия данных


	Методы сжатия данных имеют достаточно длинную историю.
Попытаемся дать краткий обзор основных идей и реализаций, не
претендующий, однако, на полноту. Более подробные сведения можно найти,
например, в [Кричевский], [Witten].

	Существует ряд "наивных" подходов к данной проблеме.  Наиболее
известный -- это кодирование длин серий (run length encoding, RLE).
Суть метода: замена цепочек повторяющихся символов на один этот символ
и счетчик повторения. Проблема состоит в том, чтобы распаковщик мог
отличить в результирующем потоке такую кодированную серию от других
символов. Решение очевидно -- снабжать все такие цепочки некоторыми
заголовками (например, использовать первый бит как признак кодированной
серии). Метод достаточно эффективен для графических изображений в
формате "байт на пиксел" (например, формат PCX использует кодирование
RLE).

	Недостатки метода RLE очевидны -- низкая степень сжатия
(например, в тексте данной статьи он сможет упаковать только цепочки
пробелов в начале строк).

	Сделаем здесь небольшое отступление для уточнения терминологии.
В дальнейшем мы будем рассматривать упаковщик (compressor) как
программу, преобразующую некоторый двоичный поток данных (подобный
файлу в системе UNIX) в другой, желательно меньшего размера.
Соответственно, распаковщик (uncompressor) -- программа, осуществляющая
обратное преобразование, причем однозначно. Таким образом, мы исключаем
из рассмотрения методы сжатия, теряющие информацию (например, метод
сжатия изображений JPEG, основанный на преобразованиях цветов,
практически не различаемых человеческим глазом).

	Под стоимостью кодирования понимается средняя длина кодового
слова (в битах). Избыточность кодирования равна разности между
стоимостью и энтропией кодирования. Очевидно, что хороший алгоритм
сжатия должен минимизировать избыточность.  Фундаментальная теорема
Шеннона о кодировании источников говорит о том, что стоимость
кодирования всегда не меньше энтропии источника, хотя может быть сколь
угодно близка к ней [Шеннон].

	Дальнейшее обсуждение этого и связанных с ним вопросов см.
[Кричевский], стр.15-54.

	Далее, процесс сжатия данных разбивается на два -- т.н.
моделирование и кодирование (см., например, [Rissanen]). Эти процессы
(и алгоритмы, их реализующие) можно рассматривать независимо.

        Вначале поговорим о технологиях кодирования.



.te 1 .ce Методы кодирования


	Кодирование (encoding) имеет дело с потоком символов в
некотором алфавите, причем частоты символов различны. Целью кодирования
является преобразование этого потока в поток бит минимальной длины. Это
достигается уменьшением энтропии потока путем учета частоты символов:
длина кода должна быть пропорциональна информации, содержащейся во
входном потоке. Если распределение вероятностей частот известно, то
можно построить оптимальное кодирование.  Задача усложняется в случае,
если распределение частот символов заранее неизвестно. В этом случае
существует два различных подхода.

	Первый подход: просмотреть входной поток и построить
кодирование на основании собранной статистики (при этом потребуются два
прохода по файлу, что ограничивает сферу применения таких алгоритмов).
В выходной поток в таком случае должна быть записана схема
использованного кодирования, которая будет использована затем
декодером. Пример -- статическое кодирование Хаффмена [Huffman].

        Второй подход использует так называемый адаптивный кодер
(adaptive coder). Идея состоит в том, чтобы менять схему кодирования в
зависимости от исходных данных. Такой алгоритм однопроходен и не
требует передачи информации об использованном кодировании в явном виде.
Вместо этого декодер, считывая кодированный поток, синхронно с кодером
изменяет схему кодирования, начиная с некоторой предопределенной.
Адаптивное кодирование может дать большую степень сжатия, поскольку
могут быть учтены локальные изменения частот.  Примером является
динамическое кодирование Хаффмена (см. [Gallager], [Knuth], [Vitter]).

        В качестве примера рассмотрим статическое кодирование Хаффмена.
Это кодирование сопоставляет входным символам (обычно представляемым
цепочками битов различной длины) цепочки битов переменной длины.  Длина
кода для символа пропорциональна двоичному логарифму его частоты,
взятому с обратным знаком. Это кодирование является префиксным, что
позволяет легко его декодировать (в префиксном кодировании код любого
символа не является префиксом кода никакого другого символа; подробнее
см. [Кричевский], 29-34).

        Пусть входной алфавит состоит из четырех символов:  a, b, c, d,
частоты которых равны соответственно 1/2, 1/4, 1/8, 1/8.  Кодирование
Хаффмена для этого алфавита дается следующей таблицей:

         символ │ частота │    входное   │  выходное
                │         │  кодирование │ кодирование
        ────────┼─────────┼──────────────┼─────────────
           a    │   1/2   │   00         │   0
           b    │   1/4   │   01         │   10
           c    │   1/8   │   10         │   110
           d    │   1/8   │   11         │   111

        Например, кодом цепочки abaaacb, представленной на входе как 00
01 00 00 00 10 01, будет 0 10 0 0 0 110 10 (пробелы добавлены для
удобства чтения). 14 бит на входе дали 11 бит на выходе. Кодирование по
Хаффмену обычно строится и хранится в виде двоичного дерева, в листьях
которого находятся символы, а на дугах "написаны" цифры 0 или 1. Кодом
символа является путь от корня дерева к этому символу.  При
использовании адаптивного кодирования Хаффмена проблема состоит в
необходимости постоянной корректировки дерева в соответствии с
изменяющейся статистикой входного потока.

        Достоинствами метода Хаффмена являются его достаточно высокая
скорость и хорошее качество сжатия. Этот алгоритм сравнительно давно
известен и широко применяется; примерами могут служить программа
compress ОС UNIX (программная реализация) и стандарт кодирования для
факсов [Hunter] (аппаратная реализация).

	Кодирование Хаффмена имеет минимальную избыточность при
условии, что каждый символ кодируется отдельной цепочкой в алфавите
{0,1}).

        Недостатком кодирования Хаффмена является зависимость степени
сжатия от близости вероятностей символов к отрицательным степеням 2;
это связано с тем, что каждый символ кодируется *целым* числом бит.
Наиболее ярко это проявляется при кодировании двухсимвольного алфавита:
в этом случае сжатие *всегда* отсутствует, несмотря на различие
вероятностей символов; алгоритм фактически "округляет" их до 1/2!

        Эта проблема может быть частично рещена за счет блокирования
входного потока (т.е. введения в рассмотрение новых символов вида 'ab',
'abc', ... где a, b, c -- символы исходного алфавита).  Однако это не
позволяет полностью избавиться от потерь (они лишь уменьшаются
пропорционально размеру блока) и приводит к резкому росту кодового
дерева: если, например, символами входного алфавита являются 8-битовые
байты со значениями 0 .. 255, то при блокировании по два символа мы
получаем 65536 символов (и столько же листьев кодового дерева), а при
блокировании по три -- 16777216!  Соответственно возрастут требования к
памяти и время построения дерева (а при адаптивном кодировании -- и
время обновления дерева, а значит, и время сжатия). Потери же составят
в среднем 1/2 бита на символ при отсутствии блокирования, а при его
наличии -- 1/4 и 1/6 бита соответственно для блоков длин 2 и 3.
Большую степень сжатия, не зависящую от близости значений вероятности
символов к степеням 1/2, может дать так называемое арифметическое
кодирование.

        Арифметическое кодирование (подробное описание см. [Witten])
является методом, позволяющим упаковывать символы входного алфавита без
потерь при условии, что известно распределение частот этих символов.
Концепция метода восходит к работам Элиаса в 60-x годах (см. [Abramson,
60-61]). В дальнейшем метод был развит и значительно усовершенствован
([Rissanen]).

	Арифметическое кодирование является оптимальным, достигая
теоретической границы степени сжатия, -- энтропии входного потока.

	Текст, сжатый арифметическим кодером, рассматривается как
некоторая двоичная дробь из интервала [0, 1). Результат сжатия можно
представить как последовательность двоичных цифр из записи этой дроби.
Идея метода состоит в следующем: исходный текст рассматривается как
запись этой дроби, где каждый входной символ является "цифрой" с весом,
пропорциональным вероятности его появления. Поясним работу метода на
примере.

	Пусть алфавит состоит из двух символов: a и b с вероятностями
соответственно 3/4 и 1/4. Как уже говорилось выше, кодирование Хаффмена
не может упаковывать слова в данном алфавите.

	Рассмотрим (открытый справа) интервал [0, 1). Разобьем его на
части, длина которых пропорциональна вероятностям символов.  В нашем
случае это [0, 3/4) и [3/4, 1).  Суть алгоритма в следующем: каждому
слову во входном алфавите соответствует некоторый подинтервал из [0,1).
Пустому слову соответствует весь интервал [0, 1). После получения
каждого последующего символа арифметический кодер уменьшает интервал,
выбирая ту его часть, которая соответствует вновь поступившему символу.
Кодом цепочки является интервал, выделенный после обработки всех ее
символов, точнее, двоичная запись любой точки из этого интервала.

	Таким образом, длина полученного интервала пропорциональна
вероятности появления кодируемой цепочки.

	Выполним алгоритм для цепочки "aaba":

 Шаг │ Просмотренная  │ Интервал
     │    цепочка     │
─────┼────────────────┼──────────────────────────────────────────────
 0.  │  ""            │ [0, 1) = [0, 1)
 1.  │  "a"           │ [0, 3/4) = [0, 0.11)
 2.  │  "aa"          │ [0, 9/16) = [0, 0.1001)
 3.  │  "aab"         │ [27/64, 36/64) = [0.011011, 0.100100)
 4.  │  "aaba"        │ [108/256, 135/256) = [0.01101100, 0.10000111)

        В качестве кода можно взять любое число из интервала,
полученного на шаге 4, например, 0.1.

        Арифметический декодер работает синхронно с кодером: начав с
интервала [0, 1), он последовательно определяет символы входной
цепочки. В частности, в нашем случае он вначале разделит
(пропорционально частотам символов) интервал [0, 1) на [0, 0.11) и
[0.11, 1).  Поскольку число 0.0111 (переданный кодером код цепочки
"aaba") находится в первом из них, можно получить первый символ: "a".
Затем делим первый подинтервал [0, 0.11) на [0, 0.1001) и [0.1001,
0.1100) (пропорционально частотам символов).  Опять выбираем первый,
так как 0 < 0.0111 < 0.1001.  Продолжая этот процесс, мы однозначно
декодируем все четыре символа.  Для того, чтобы декодер мог определить
конец цепочки, мы можем либо передавать ее длину отдельно, либо
добавить к алфавиту дополнительный символ "конец цепочки".

        При рассмотрении этого метода возникают две проблемы:
во-первых, необходима вещественная арифметика, вообще говоря,
неограниченной точности, и во-вторых, результат кодирования становится
известен лишь при окончании входного потока.  Дальнейшие исследования,
однако, показывают [Rubin], что можно практически без потерь обойтись
целочисленной арифметикой небольшой точности (16-32 разряда), а также
добиться инкрементальной работы алгоритма: цифры кода могут выдаваться
последовательно по мере чтения входного потока.



.te 1 .ce Модели входного потока


	Как говорилось выше, кодирование представляет собой лишь часть
процесса упаковки. Не менее важно т.н. моделирование (modelling).  Мы
уже говорили о том, что арифметическое кодирование имеет минимальную
избыточность при заданном распределении. Чудесно! -- воскликнет
читатель. Но откуда берется это распределение? И о каком алфавите идет
речь?

	Ответы на эти вопросы дает модель потока, представляющая собой
(см. [Rissanen], [Witten]) некоторый способ определения распределения
вероятностей при каждом поступлении очередного символа. Именно каждом,
поскольку статические модели (в которых распределение не изменяется) в
большинстве случаев не дают максимального качества сжатия. Гораздо
больший интерес представляют так называемые адаптивные модели,
учитывающие текущий контекст.  Такие модели позволяют строить быстрые
однопроходные алгоритмы сжатия, не требующие априорных знаний о входном
потоке данных и строящие расределение "на лету". Выделяют также класс
"локально адаптивных" алгоритмов, отдающих при построении распределения
предпочтение последним поступившим символам.

	Возможны различные подходы к этой проблеме: простейший из них
-- сбор статистики появления каждого символа независимо от других
(моделирование источником Бернулли: вероятность появления символа не
зависит от того, какие символы встретились перед ним). Возможно также
использование марковской модели: сбор статистики появления каждого
символа с учетом некоторого количества предыдущих символов (в
марковском источнике первого порядка вероятность появления символа
зависит только от одного последнего полученного символа, и т.д.).
Марковские модели могут давать более точную картину источника, однако
число состояний в них больше, соответственно больше обьем хранимых
таблиц частот. Кроме того, при использовании кодирования Хаффмена они
могут даже ухудшить качество сжатия, поскольку порождаемые ими
вероятности обычно хуже приближаются степенями 1/2.

	Говоря о моделях входного потока и адаптивных алгоритмах
сжатия, нельзя не упомянуть простой и достаточно эффективный метод
кодирования источника с неизвестным распределением частот, известный
как сжатие при помощи стопки книг. Метод был впервые открыт и
исследован Рябко в 1980 г. (см. [Рябко]), а затем переоткрыт Бентли,
Слейтером, Тарьяном и Веи в 1986 г. (см. [Bentley]).

	Идея метода состоит в следующем: пусть алфавит источника
состоит из N символов с номерами 1, 2, ..., N. Кодер хранит список
символов, представляющий собой некоторую перестановку алфавита. При
поступлении на вход символа c, имеющего в этос списке номер i, кодер
передает код номера i (например, монотонный код: [Кричевский],
стр.69-73). Эатем кодер переставляет символ c в начало списка,
увеличивая на 1 номера всех символов, стоящих перед c. Таким образом,
более "популярные" символы будут тяготеть к началу списка и иметь более
короткие коды.



.te 1 .ce Двухступенчатое кодирование. Алгоритм Лемпеля-Зива


	Все рассмотренные выше методы и модели кодирования
рассматривали в качестве входных данных цепочки символов (тексты) в
некотором конечном алфавите. При этом оставался открытым вопрос о связи
этого входного алфавита кодера с данными, подлежащими упаковке (обычно
также представленными в виде цепочек в алфавите, обычно состоящем из
256 символов-байт).

	В простейшем случае можно использовать в качестве входного
алфавита кодера именно эти символы (байты) входного потока. Именно так
работает метод squashing программы PKPAK (использовано статическое
кодирование Хаффмена, двухпроходный алгоритм). Степень сжатия при этом
относительно невелика -- порядка 50% для текстовых файлов.

	Гораздо большей степени сжатия можно добиться при выделении из
входного потока повторяющихся цепочек и кодирования ссылок на эти
цепочки.

	Этот метод, о котором и пойдет далее речь, принадлежит Лемпелю
и Зиву (см. [Lempel), и обычно называется LZ-compression. Суть его в
следующем: упаковщик постоянно хранит некоторое количество последних
обработанных символов в буфере.  По мере обработки входного потока
вновь поступившие символы попадают в конец буфера, сдвигая
предшествующие символы и вытесняя самые старые.  Размеры этого буфера,
называемого также скользящим словарем (sliding dictionary), варьируются
в разных реализациях; экспериментальным путем удалось установить, что
программы LHarc 1.13 используeт 4-килобайтный буфер, LHA 2.13 и PKZIP
1.10 -- 8-килобайтный, а ARJ 2.20 -- 16-килобайтный.

	Алгоритм выделяет (путем поиска в словаре) самую длинную
начальную подстроку входного потока, совпадающую с одной из подстрок в
словаре, и выдает на выход пару (length, distance), где length -- длина
найденной в словаре подстроки, а distance -- расстояние от нее до
входной подстроки (то есть фактически индекс подстроки в буфере,
вычтенный из его размера). В случае, если такая подстрока не найдена, в
выходной поток просто копируется очередной символ входного потока.

	В первоначальной версии алгорима предлагалось использовать
простейший поиск по всему словарю. Время сжатия при такой реализации
было пропорционально произведению длины входного потока на размер
буфера, что непригодно для практического использования. Однако, в
дальнейшем было предложено испольэовать двоичное дерево для быстрого
поиска в словаре [Bell], что поэволило на порядок поднять скорость
работы алгоритма.

	Таким образом, алгоритм Лемпеля-Зива преобразует один поток
исходных символов в два параллельных потока length и distance.
Очевидно, что эти потоки являются потоками символов в новых алфавитах L
и D, и к ним можно применить один из упоминавшихся выше методов (RLE,
кодирование Хаффмена, арифметическое кодирование).  Так мы приходим к
схеме двухступенчатого кодирования, наиболее эффективной из практически
используемых в настоящее время (и, в частности, использованной
автором). При реализации этого метода необходимо добиться
согласованного вывода обоих потоков в один файл. Эта проблема обычно
решается путем поочередной эаписи кодов символов из обоих потоков.

	Следует также отметить широко известный алгоритм
Лемпеля-Зива-Велча (Lempel-Ziv-Welch compression, LZW).  Алгоритм
отличают высокая скорость работы как при упаковке, так и при
распаковке, достаточно скромные требования к памяти и простая
аппаратная реализация. Недостаток -- низкая степень сжатия по сравнению
со схемой двухступенчатого кодирования. Кратко опишем алгоритм. Более
подробные сведения см. [Welch].

	Создадим словарь, хранящий строки текста и содержащий порядка
2-4-8 тысяч пронумерованных гнезд. Запишем в первые 256 гнезд строки,
состоящие из одного символа, номер которого равен номеру гнезда.
Алгоритм просматривает входной поток, разбивая его на подстроки и
добавляя новые гнезда в конец словаря.

	Прочитаем несколько символов в строку s и найдем в словаре
строку t -- самый длинный префикс s. Пусть он найден в гнезде с номером
n.  Выведем число n в выходной поток, переместим указатель входного
потока на length (t) символов вперед и добавим в словарь новое гнездо,
содержащее строку t+c, где с -- очередной символ на входе (сразу после
t). Алгоритм преобразует поток символов на входе в поток индексов ячеек
словаря на выходе. При размере словаря в 4096 гнезд можно передавать 12
бит на каждый индекс.

	Каждая распознанная цепочка добавляет в словарь одно гнездо.
При переполнении словаря упаковщик может либо прекратить его
заполнение, либо очистить (полностью или частично).

	При практической реализации этого алгоритма следует учесть, что
любое гнездо словаря, кроме самых первых, содержащих односимвольные
цепочки, хранит копию некоторого днугого гнезда, к которой в конец
приписан один символ. Вследствие этого можно обойтись простой списочной
структурой с одной связью.



.te 1 .ce Проектирование. Использованные алгоритмы


	Перейдем теперь к описанию алгоритмов, использованных в нашей
реализации. Но сначала -- несколько слов о целях, поставленных при
разработке и определивших принятые решения.

	Прежде всего, упаковшик разрабатывался с целью встраивания его
в программы пользователя. Поэтому не потребовалась разработка
специального формата архивного файла. Достаточно лишь пребразователя
"файл в файл" (аналогично программе compress в системе UNIX). Далее,
накладывались достаточно жесткие ограничения на используемую память:
реализованный алгоритм использует 44K для упаковки и всего 12K для
распаковки (не считая обьема кода, однако он тоже незначителен), в то
время как PKZIP требует 90K и 70K соответственно, ARJ -- 275K и 160K).
Далее, необходима высокая скорость распаковки; на скорость же упаковки
накладываются более мягкие ограничения.

	Исходя из этого, был использован алгоритм Лемпеля-Зива с
размером скользящего словаря 4 килобайта, что позволило минимизировать
требования к памяти без существенных потерь на степени сжатия.  Для
кодирования длин и расстояний использовалось статическое кодирование
Хаффмена со специально подобранным кодовым деревом, оптимизированным
для текстовых файлов.  Кроме того, был использован быстрый
байт-ориентированный алгоритм распаковки, позволяющий уменьшить число
побитовых пересылок и соответственно увеличить скорость работы.

	Статистика, собранная при разработке программы, показала, что
более 97% всех обнаруженных цепочек имеют длину, не превосходящую 16
символов. Это позволило без существенных потерь степени сжатия обойтись
быстрым деревом с 16 листьями. Цепочки с длиной len, где len >= 16,
кодировались как цепочки длины 16 с последующим дополнительным байтом
"расширения длины" со значением (len - 16).

	Далее, для расстояний (distances) цепочек кодировались их
старшие байты, значения которых находились в диапазоне [0..15] (ввиду
размера буфера 4096 байт). Младшие байты не кодировались ввиду
сравнительно небольшого "уклона" распределения частот.

	Одиночные символы (цепочки длины 1) кодировались как цепочки
длины 1 с последующим байтом, содержащим сам символ. Следует отметить,
что цепочки длины 2 кодировались именно как цепочки, а не как две
отдельные литеры. Это давало средний выигрыш в 3 бита на символ: (2 + 8
+ 2 + 8) = 20 бит при 2 цепочках по 2 символа и (2 + 4 + 8) = 14 бит
при одной дыухсимвольной цепочке. Далее, небольшая модификация
алгоритма поиска совпадающих подстрок позволила эффективно упаковывать
серии повторяющихся цепочек символов:  при поиске символов в буфере
искомая цепочка неявно довавлялась в конец буфера:  например, цепочка
"abcabcabcabcabc" кодировалась как (1, "a"), (1, "b"), (1, "c"), (12,
3) -- фактически через обращение к самой себе со сдвигом на 3 символа.



.te 1 .ce Реализация и структура программы


	Сжатие входного потока осуществляется функцией SquoPack(),
получающей на входе адреса пользовательских функций ввода-вывода.
Основной цикл программы может быть представлен следующим образом:


SquoPack()	{
	Init (); // init dictionary, trees, and output bit buffer

	while (not EOF)	{
		int	length;
		int	distance;

		FindMatch (&length, &distance);
		// scan input and find longest string in dictionary
		// which matches with it; record its length and distance
		// from the end of dictionary.

		EncodeLength (length);
		EncodeDistance (distance);

		UpdateDictionary (length);
		// this discards first 'length' chars
		// from the beginning of dictionary
		// and reads next characters from input
	}

	Done (); // output EOF marker and flush output buffers.
}


	Словарь (dictionary) обеспечивает хранение 4096 последних
считанных символов, до 271(=16+255) вновь поступивших на вход символов
и быстрый поиск подстроки, совпадающей с входной. При реализации было
использовано хеширование с разрешением конфликтов путем хранения
односвязных списков.

	Действие FindMatch реализовано непосредственно в функции
SquoPack(); действие UpdateDictionary реализовано функцией MoveBuf().
Кодирование длин и расстояний и вывод полученных кодов осуществляют
соответственно функции PutLen() и PutDist(). Они, в свою очередь,
обращаются к функциям PutBits() и PutByte() для побайтной буферизации
выводимых цепочек бит. Применена динамическая перестановка вершин
дерева Хаффмена по модифицированному алгоритму "стопки книг" (см.
[Рябко]).

	Распаковщик практически полностью реализован на
в виде одной ассемблерной функции _SquoUnpack.



.te 1 .ce Профилирование и оптимизация


	Первоначальная версия упаковщика была написана и отлажена
практически полностью на си (распаковщик был с самого начала был
реализован на ассемблере). Анализ результатов профилирования позволил
за счет оптимизации функций PutLen() и PutDist() и переделки функции
UpdateHash() в макрос уменьшить время сжатия примерно на 18%.  Далее,
статистика показала, что строки длиной 2, 3 и 4 символа составляют
около 75% общего количества (не считая строк длины 1, т.е.  одиночных
символов).  Переход к другому методу хеширования (по четырем первым
символам) позволил дополнительно увеличить скорость работы программы и
уменьшить требования к памяти.  Дальнейшая оптимизация связана с
переписыванием критичных по времени участков кода на язык ассемблера
(модуль распаковки был написан на ассемблере изначально).

	Протокол, содержащий результаты пробных запусков программы,
а также а также отчет профилировщика, приведен в части 2.



.te 1 .ce Руководство пользователя


Компиляция и сборка:
====================

	Пользователю предлагаются две функции, позволяющие реализовать
соответственно упаковку и распаковку двоичных потоков данных (т.е.,
сжатие при использовании последовательного ввода-вывода). Описания
функций находятся в файле squo.h, определения -- в файлах squo.c,
lzp.asm, lzu.asm. Для сборки программы пльзователя необходимо
аодключить соответствующие обьектные модули (squo.obj, lzp.obj,
lzu.obj).  Для компиляции этих обьектных модулей необходимы файлы
style.h, squo.h, squo.c, lz.asi, lzp.asm, lzu.asm.  Тестовая программа
(простейший упаеовщик для файлов, аналогичный compress) находится в
файле squotest.c.

	При разработке, отладке и тестировании использовался компилятор
Turbo C 2.01, модель памяти compact. Перенос программы на другой
си-компилятор для MS-DOS не должен представлять сложностей; для других
архитектур, очевидно, понадобится переписывание модулей на языке
ассемблера. Поскольку их объем невелик, а функции тривиальны, это также
не должно вызывать больших трудностей. В настоящее время автор работает
над UNIX-мобильной версией программы.


Интерфейс вызова:
=================

#include "squo.h"

extern	int	SquoPack (ReadFunc *reader, WriteFunc *writer);

extern	int	SquoUnpack (ReadFunc *reader, WriteFunc *writer);


Возвращаемое значение:
======================

	Эначение 0 обозначает успешное завершение, 1 -- неуспешное.


Параметры:
==========

	Параметры reader и writer -- соответственно указатели на
функции ввода и вывода, определяемые пользователем. Для функции
SquoPack, например, reader должен поставлять неупакованные данные
(скажем, передавать их из массива в памяти), а функции writer эти
данные будут переданы уже упакованными (например, для записи в файл).
Подробности -- в демонстрационном примере squotest.c.

	Типы ReadFunc и WriteFunc описаны в заголовочном файле squo.h:

typedef	size_t	ReadFunc (void * buffer, size_t size);
typedef	size_t	WriteFunc (void * buffer, size_t size);

	ReadFunc читает (WriteFunc соответственно записывает) size байт
(size <= 65535) по адресу buffer. Возвращаемое значение -- число
прочитанных (записанных) байт.


Ошибки:
=======

	В настоящей версии игнорируются ошибки вывода при упаковке и
распаковке. Впрочем, подобные средства могут быть реализованы в
программе пользователя. Распаковщик не контролирует правильность
входных данных (неожиданное окончание ввода).




.te 1 .ce Использованные источники


Кричевский, Р.Е. Сжатие и поиск информации. - М.: Радио и связь, 1989.
Теоретическая работа, посвященная теории кодирования источника.
Содержит многие важные результаты, в том числе оценки сложности
алгоритмов кодирования. Большое внимание уделено универсальным кодам.

Рябко Б.Я. Сжатие информации с помощью стопки книг // Проблемы передачи
информации. - 1980. - Т.16, N.4. - С. 16-21. Первое описание локально
адаптивного метода стопки книг.

Шеннон К. Работы по теории информации и кибернетике. - М.: ИЛ, 1963.
Изложены классические результаты теории информации.

Abramson, N. Information Theory and Coding. McGraw-Hill, New York,
1963. Первая ссылка на метод, позже названный арифметическим
кодированием (стр. 61-62).

Bell, T.C. IEEE Trans. COM-34, pp. 1176-1182 (1986).
Применение двоичного дерева поиска к алгоритму Лемпеля-Зива.

Bentley, J.L., Sleator, D.D., Tarjan, R.E., Wei, V.K. A locally
adaptive data compression scheme. Commun. ACM 29, 4 (Apr. 1986),
320-330. Описан локально адаптивный алгоритм сжатия, использующий
эвристику "двигай вверх" (алгоритм стопки книг Рябко).

Gallager, R.G. Variations on the theme by Huffman. IEEE Trans. Inf.
Theory IT-24, 6 (Nov. 1978), 668-674. Эффективный алгоритм адаптивного
кодирования Хаффмена.

Huffman, D.A. A method for the construction of minimum-redundancy
codes. Proc. Inst. Electr. Radio Eng. 40, 9 (Sept. 1952), 1098-1101.
Классическая работа, впервые описывающая кодирование Хаффмена.

Hunter, R. and Robinson, A.H. International digital fscsimile coding
standarts. Proc. Inst. Electr. Electron. Eng, 68, 7 (July 1980),
854-867. Описано приложение кодирования Хаффмена к сжатию длин серий в
черно-белых изображениях.

Knuth, D.E. Dynamic Huffman coding. J. Algorithms 6, 2 (Feb. 1985),
163-180. Практическая реализация локально адаптивного кодирования
Хаффмена.

Rissanen, J.J. Arithmetic codings as number representations. Acta
Polytech. Scand. Math. 31 (Dec. 1979), 44-51. Дальнейшее развитие идей
арифметического кодирования.

Rissanen, J., and Langdon, G.G. Universal modelling and coding. IEEE
Trans. Inf. Theory IT-27, 1 (Jan 1981), 12-23. Показана возможность
разделения сжатия данных на моделирование и кодирование.

Rubin, F. Arithmetic stream coding using fixed precision registers.
IEEE Trans. Inf. Theory IT-25, 6 (Nov. 1979), 672-675. Одна из первых
работ, включающая все необходимые элементы арифметического кодирования:
вычисления с фиксированной точкой и инкрементальная работа алгоритма.

Vitter, J.S. Two papers on dynamic Huffman codes. Tech. Rep. CS- 85-13.
Brown University Computer Science. Providence, R.I. Revised Dec. 1986.
Оптимальное адаптивное кодирование Хаффмена.

Welch, T.E. A technique for high-perfomance data compression.  IEEE
Comput. 17, 6 (June 1984), 8-19. Описан быстрый алгоритм сжатия данных,
не отличающийся, однако, высоким качеством сжатия. Алгоритм известен
как LZW (Lempel-Ziv-Welch compression) и достаточно популярен
(графический формат GIF, метод shrinking программы PKZIP).

Witten, I.H., Neal, R.M., and Cleary, J.G. Arithmetic coding for data
compression. Commun. ACM 30, 6 (June 1987), 520-540. Классическая
работа по арифметическому кодированию.

Ziv, J., and Lempel, A. Compression of individual sequences via
variable-rate coding. IEEE Trans. Inf. Theory IT-24, 5 (Sept. 1978),
530-536. Описан метод сжатия, заменяющий подстроку текста указателем на
ее более раннее вхождение. На этих идеях основано большинство
практически используемых программ сжатия данных.



.te 1 .ce Приложение А. Обзор программ сжатия для MS-DOS.


	В следующей таблице содержатся результаты тестирования
программы на нескольких файлах. Для сравнения приведены результаты
для трех популярных архиваторов для MS-DOS. Левая колонка содержит
длину сжатого файла, правая -- отношение размера сжатого файла к
размеру оригинального файла. Файл squo.c представляет собой
исходный текст одного из модулей описываемой программы, файл mawk.doc
-- текст на английском языке, файл sos.doc -- текст на русском языке.
В колонке 'Original' приведены размеры неупакованных файлов.


Filename   Original SquoTest 1.5 PKZIP 1.0   LHA 2.13     ARJ 2.30
---------  -------- ------------ ----------- ------------ -----------
SQUO.C        17821    5876  33%   5371  30%   5148  29%    5089  29%
MAWK.DOC      42961   17108  40%  15359  35%  15122  35%   14713  34%
SOS.DOC       28525   14373  50%  14356  50%  12957  45%   12500  44%


	Далее приводится перечень (эаведомо неполный) программ сжатия
для MS-DOS с кратким указанием алгоритмов их работы и дальнейшими
ссылками.  Автор выражает благодарности Haruhiko Okumura ('Data
Compression Algorithms of LARC and LHarc'), Robert K Jung (ARJ archiver
documentation), PKWARE Inc. (PKZIP archiver documentation) эа
предоставленную информацию.


PKPAK 3.61:

Метод Packed -- алгоритм RLE.
Метод Crunched -- алгоритм LZW.
Метод Squashed -- двухпроходное статическое кодирование Хаффмена.

PKZIP 1.10:

Метод Shrinked -- модифицированный алгоритм LZW с частичной очисткой
словаря и переменной длиной кода.

Метод Imploded -- модифицированный алгоритм Лемпеля-Зива и
статическое кодирование Хаффмена.

LHArc:
Алгоритм Лемпедя-Зива и динамическое кодирование Хаффмена.

LHA:
Алгоритм Лемпедя-Зива и статическое кодирование Хаффмена.

ARJ:
Алгоритм Лемпеля-Зива и оригинальный метод кодирования (источник:
авторская документация).


