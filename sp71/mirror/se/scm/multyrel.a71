From: nci@pipeline.com (Network Concepts)
Newsgroups: comp.software.config-mgmt
Subject: Re: Re:
Date: 20 Jan 1995 09:02:15 -0500
Organization: The Pipeline
Message-ID: <3fofp7$qcd@pipe3.pipeline.com>

Jean Stanford writes: 
 
(snip) 
 
 
>Yes, this is the situation that we find ourselves in frequently.  In the
client-server environment, it is more >and more common.  This raises real
questions about maintenance of mission-critical systems. If you don't
>capture the tools which were used to create the actual GUI or generate the
source or whatever, you >won't be able to recreate the version and you
won't be able to fix the versions in the field. 
 
>There is one school of thought that says "so what, re-generate a new
version".  The people advocating >that school of thought have usually been
supporting small, non-critical systems.  We can't just tweak a >system with
1000 or more users and blythely hope it will work. 
 
>One solution is to archive the ENTIRE environment: all the tools, all the
scripts, all the libraries, etc.  >Again, this doesn't work too well in
real life.  Media decays over time, people forget how to set things up >so
that they can find the actual thing they want to fix, the people who know
how to use the tool all go off >to work on other projects (especially if
we've "saved money" by contracting out lots of the work). 
 
>I hope that someone out there has a great answer to this one! All the
ideas I have are expensive... 
 
(snip) 
 
>Regards, Jean Stanford 
 
Jean, you hit the mark on this.  My company is a vendor of software
products for mission critical systems. 
The software is used on a single computer vendor's proprietary hardware and
operating systems.  
 
One of the products (an SCM tool, interestingly enough) has been offered
for twelve years.  Nearly 
five hundred customers use this tool.  When a new "baseline" release
appears it takes about 18 months 
before most of the clients upgrade to it and there are always a few who,
effectively, never upgrade.  Over 
time, we have learned how to deal with this, but we usually are supporting
8-12 releases in the field at 
any moment. 
 
Over the years, the computer vendor has released five new hardware
"families,"  about six major O.S.  
versions and dozens of dot releases. 
 
As a result, there are a staggering number of combinations that we might
have to track. Each one requires  
that we save the ENTIRE environment, just as you said.  To keep things in
semi-reason, we have  
developed a few "rules-of-thumb." 
   1. We try to change our "build" tools as infrequently as we can.  (Each
time we change compilers we 
       have to build and test current and back versions to see if we can
use that tool and create an 
       unchanged result.)   
   2. After we ugrade the O.S. on our development system, we test the
earlier sets of build tools to see if 
       they function and make the same result.  If not, the old version of
the O.S. becomes the cutoff point 
       for use of that tool set. 
   3. We require that our products always be built by an "automated"
command stream, no operator entries. 
       (If we have to restore an old environment, we don't want to have to
try to remember how we built the 
       product back then.  The command stream is archived and versioned to
help the process.) 
   4. Finally, when we can no longer build and deliver the version, we
inform the affected customers. 
 
Of course, off-site copies are essential for disaster backup, etc. 
 
This all costs money.  I have to chuckle sometimes when a client says, "Why
do I have to pay for sw 
maintenance when I'm not on the latest version?"   
 
I have enjoyed the discussion in this group.  Like you, I'm hoping that in
an exchange of ideas and  
experiences, I can learn to do this work better, and more cost-effectively.

 
Dick Ward 
nci@pipeline.com   
         
   
    

